{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba268f7b",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d273486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file 'Data_2.txt' in read mode and read its content into the variable 'sentence'\n",
    "with open(\"Data_2.txt\", 'r') as file:\n",
    "    sentence = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "439db3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The big black dog barked at the white cat and chased away.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the content of the file\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c978954",
   "metadata": {},
   "source": [
    "# NLTK POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d833815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary NLTK modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e96ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of English stopwords using NLTK\n",
    "py_sword = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7b96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into sentences using NLTK\n",
    "py_token = sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66d731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK POS Tagger:\n",
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Print the NLTK POS Tagger output for each sentence\n",
    "print(\"NLTK POS Tagger:\")\n",
    "for i in py_token:\n",
    "    py_lword = nltk.word_tokenize(i)\n",
    "    # Perform POS tagging using NLTK\n",
    "    py_tag = nltk.pos_tag(py_lword)\n",
    "    print(py_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc92fd9",
   "metadata": {},
   "source": [
    "# TextBlob POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af56bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TextBlob module\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e86826f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TextBlob object for the text\n",
    "blob_object = TextBlob(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec418f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob POS Tagger:\n",
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# Print the TextBlob POS Tagger output\n",
    "print('TextBlob POS Tagger:')\n",
    "print(blob_object.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a151d8b",
   "metadata": {},
   "source": [
    "# Regular Expression Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d236fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary NLTK modules\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import RegexpTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72bd4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patterns for POS tagging using regular expressions\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),             \n",
    "    (r'.*ed$', 'VBD'),              \n",
    "    (r'.*es$', 'VBZ'),              \n",
    "    (r'.*ould$', 'MD'),             \n",
    "    (r'.*\\'s$', 'NN$'),             \n",
    "    (r'.*s$', 'NNS'),               \n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "    (r'^\\d+$', 'CD'),               \n",
    "    (r'.*ing$', 'VBG'),             \n",
    "    (r'.*ment$', 'NN'),             \n",
    "    (r'.*ful$', 'JJ'),\n",
    "    (r'\\b(?:and)\\b', 'CC'),\n",
    "    (r'\\b(?:away)\\b', 'RB'),\n",
    "    (r'(The|the|A|a|An|an)$', 'AT'),\n",
    "    (r'\\b(?:big|black|white)\\b', 'JJ'),\n",
    "    (r'\\b(?:at)\\b', 'IN'),\n",
    "    (r'\\.$', 'PUN'),\n",
    "    (r'.*', 'NN'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b0ff7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RegexpTagger using the defined patterns\n",
    "tagger = nltk.tag.sequential.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c53059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text using NLTK word tokenizer\n",
    "tokenized_text = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687837f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex Tagger:\n",
      "[('The', 'AT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'AT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', 'PUN')]\n"
     ]
    }
   ],
   "source": [
    "# Print the output of the Regex Tagger\n",
    "print(\"Regex Tagger:\")\n",
    "# Perform POS tagging using the Regex Tagger\n",
    "print(tagger.tag(tokenized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807185ca",
   "metadata": {},
   "source": [
    "# Parse Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e03bba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary NLTK module\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bf95714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text to lowercase\n",
    "sentence_lower = sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "144f3dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the big black dog barked at the white cat and chased away.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39198068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a regular expression tokenizer to tokenize words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c32dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the lowercased sentence\n",
    "tokens = tokenizer.tokenize(sentence_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc9d5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tokens back into a cleaned sentence\n",
    "sentence_cleaned = ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23f8c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CFG for parsing sentences\n",
    "text2 = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> DT NOM | DT NN\n",
    "NOM -> ADJ NOM | ADJ NN | ADJ ADJ NN\n",
    "VP -> VP Conj VP | VB RB | VB PP\n",
    "PP -> IN NP\n",
    "DT -> 'the'\n",
    "Conj -> 'and'\n",
    "NN -> 'dog' | 'cat'\n",
    "ADJ -> 'big' | 'black' | 'white'\n",
    "RB -> 'away'\n",
    "IN -> 'at'\n",
    "VB -> 'barked' | 'chased'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08b3cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the cleaned sentence\n",
    "text1 = nltk.tokenize.word_tokenize(sentence_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bb6a9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'big',\n",
       " 'black',\n",
       " 'dog',\n",
       " 'barked',\n",
       " 'at',\n",
       " 'the',\n",
       " 'white',\n",
       " 'cat',\n",
       " 'and',\n",
       " 'chased',\n",
       " 'away']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b313647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a chart parser with the defined CFG\n",
    "parser = nltk.ChartParser(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55e0e10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (DT the) (NOM (ADJ big) (ADJ black) (NN dog)))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VB barked)\n",
      "      (PP (IN at) (NP (DT the) (NOM (ADJ white) (NN cat)))))\n",
      "    (Conj and)\n",
      "    (VP (VB chased) (RB away))))\n",
      "                              S                                            \n",
      "          ____________________|____________                                 \n",
      "         |                                 VP                              \n",
      "         |                         ________|________________________        \n",
      "         |                        VP                    |           |      \n",
      "         |               _________|___                  |           |       \n",
      "         |              |             PP                |           |      \n",
      "         |              |      _______|____             |           |       \n",
      "         NP             |     |            NP           |           |      \n",
      "  _______|____          |     |    ________|____        |           |       \n",
      " |           NOM        |     |   |            NOM      |           VP     \n",
      " |    ________|____     |     |   |         ____|___    |      _____|___    \n",
      " DT ADJ      ADJ   NN   VB    IN  DT      ADJ       NN Conj   VB        RB \n",
      " |   |        |    |    |     |   |        |        |   |     |         |   \n",
      "the big     black dog barked  at the     white     cat and  chased     away\n",
      "\n",
      "(S\n",
      "  (NP (DT the) (NOM (ADJ big) (NOM (ADJ black) (NN dog))))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VB barked)\n",
      "      (PP (IN at) (NP (DT the) (NOM (ADJ white) (NN cat)))))\n",
      "    (Conj and)\n",
      "    (VP (VB chased) (RB away))))\n",
      "                                  S                                            \n",
      "          ________________________|____________                                 \n",
      "         |                                     VP                              \n",
      "         |                             ________|________________________        \n",
      "         |                            VP                    |           |      \n",
      "         |                   _________|___                  |           |       \n",
      "         NP                 |             PP                |           |      \n",
      "  _______|____              |      _______|____             |           |       \n",
      " |           NOM            |     |            NP           |           |      \n",
      " |    ________|____         |     |    ________|____        |           |       \n",
      " |   |            NOM       |     |   |            NOM      |           VP     \n",
      " |   |         ____|___     |     |   |         ____|___    |      _____|___    \n",
      " DT ADJ      ADJ       NN   VB    IN  DT      ADJ       NN Conj   VB        RB \n",
      " |   |        |        |    |     |   |        |        |   |     |         |   \n",
      "the big     black     dog barked  at the     white     cat and  chased     away\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the tokenized sentence using the CFG and print the parse trees\n",
    "for tree in parser.parse(text1):\n",
    "    print(tree)\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b77fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
