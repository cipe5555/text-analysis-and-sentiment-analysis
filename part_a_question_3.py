# -*- coding: utf-8 -*-
"""Part A - Question 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kxeIFbB-oluCRn0LWhsxHGCpsPAoBR8O

# Reading Data
"""

# Open the file 'Data_2.txt' in read mode and read its content into the variable 'sentence'
with open("Data_2.txt", 'r') as file:
    sentence = file.read()

# Print the content of the file
sentence

"""# NLTK POS Tagger"""

# Import necessary NLTK modules
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize

# Create a set of English stopwords using NLTK
py_sword = set(stopwords.words('english'))

# Tokenize the text into sentences using NLTK
py_token = sent_tokenize(sentence)

# Print the NLTK POS Tagger output for each sentence
print("NLTK POS Tagger:")
for i in py_token:
    py_lword = nltk.word_tokenize(i)
    # Perform POS tagging using NLTK
    py_tag = nltk.pos_tag(py_lword)
    print(py_tag)

"""# TextBlob POS Tagger"""

# Import the TextBlob module
from textblob import TextBlob

# Create a TextBlob object for the text
blob_object = TextBlob(sentence)

# Print the TextBlob POS Tagger output
print('TextBlob POS Tagger:')
print(blob_object.tags)

"""# Regular Expression Tagger"""

# Import necessary NLTK modules
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import RegexpTagger

# Define patterns for POS tagging using regular expressions
patterns = [
    (r'.*ing$', 'VBG'),
    (r'.*ed$', 'VBD'),
    (r'.*es$', 'VBZ'),
    (r'.*ould$', 'MD'),
    (r'.*\'s$', 'NN$'),
    (r'.*s$', 'NNS'),
    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),
    (r'^\d+$', 'CD'),
    (r'.*ing$', 'VBG'),
    (r'.*ment$', 'NN'),
    (r'.*ful$', 'JJ'),
    (r'\b(?:and)\b', 'CC'),
    (r'\b(?:away)\b', 'RB'),
    (r'(The|the|A|a|An|an)$', 'AT'),
    (r'\b(?:big|black|white)\b', 'JJ'),
    (r'\b(?:at)\b', 'IN'),
    (r'\.$', 'PUN'),
    (r'.*', 'NN'),
]

# Create a RegexpTagger using the defined patterns
tagger = nltk.tag.sequential.RegexpTagger(patterns)

# Tokenize the text using NLTK word tokenizer
tokenized_text = word_tokenize(sentence)

# Print the output of the Regex Tagger
print("Regex Tagger:")
# Perform POS tagging using the Regex Tagger
print(tagger.tag(tokenized_text))

"""# Parse Trees"""

# Import necessary NLTK module
from nltk.tokenize import RegexpTokenizer

# Convert the text to lowercase
sentence_lower = sentence.lower()

sentence_lower

# Initialize a regular expression tokenizer to tokenize words
tokenizer = RegexpTokenizer(r'\w+')

# Tokenize the lowercased sentence
tokens = tokenizer.tokenize(sentence_lower)

# Join the tokens back into a cleaned sentence
sentence_cleaned = ' '.join(tokens)

# Define a CFG for parsing sentences
text2 = nltk.CFG.fromstring("""
S -> NP VP
NP -> DT NOM | DT NN
NOM -> ADJ NOM | ADJ NN | ADJ ADJ NN
VP -> VP Conj VP | VB RB | VB PP
PP -> IN NP
DT -> 'the'
Conj -> 'and'
NN -> 'dog' | 'cat'
ADJ -> 'big' | 'black' | 'white'
RB -> 'away'
IN -> 'at'
VB -> 'barked' | 'chased'
""")

# Tokenize the cleaned sentence
text1 = nltk.tokenize.word_tokenize(sentence_cleaned)

text1

# Initialize a chart parser with the defined CFG
parser = nltk.ChartParser(text2)

# Parse the tokenized sentence using the CFG and print the parse trees
for tree in parser.parse(text1):
    print(tree)
    tree.pretty_print()

